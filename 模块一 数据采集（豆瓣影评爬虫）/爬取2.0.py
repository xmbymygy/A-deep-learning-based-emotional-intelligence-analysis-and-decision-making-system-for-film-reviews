import re
import time
import random
import requests
import pymysql
from bs4 import BeautifulSoup
from pymysql.cursors import DictCursor
from loguru import logger
from functools import wraps

# --------------------------
# MySQLé…ç½®ï¼ˆå¯†ç å·²æ›´æ–°ï¼‰
# --------------------------
DB_CONFIG = {
    "host": "localhost",
    "user": "root",
    "password": "ygy20060227-",  # ç”¨æˆ·æä¾›çš„MySQLå¯†ç 
    "database": "movie_comment_db",
    "charset": "utf8mb4"
}

# --------------------------
# è±†ç“£Cookieï¼ˆç”¨æˆ·æä¾›çš„æœ‰æ•ˆCookieï¼‰
# --------------------------
DOUBAN_COOKIE = 'bid=MkBgKtLgqm4; _vwo_uuid_v2=D5BB088E30628E9E4EB2EDCD6EB0DF2A2|c343bc9ef9f151da8446dddfaea28959; viewed="35598244_35231456"; ll="118408"; _pk_id.100001.4cf6=84308ba5eca49734.1761122330.; __yadk_uid=ro5hsvGm9DIbYZy3FwYpIoDBwVcr9DbZ; dbcl2="282442856:kadpFn7XkjM"; push_noty_num=0; push_doumail_num=0; __utmv=30149280.28244; ck=FJS6; ap_v=0,6.0; __utmc=30149280; __utmc=223695111; __utma=30149280.760081204.1743661532.1761226591.1761229198.8; __utmz=30149280.1761229198.8.6.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; __utma=223695111.1534616943.1761122333.1761226591.1761229198.6; __utmb=223695111.0.10.1761229198; __utmz=223695111.1761229198.6.4.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1761229198%2C%22https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3DbeLvml93fNEFN4cU4AOFgqgNh9aI6zlCgwrGPROKsSuZ0BNkb-K4V1idZfuXyQrc%26wd%3D%26eqid%3De91241bc00059d890000000268fa3984%22%5D; _pk_ses.100001.4cf6=1; frodotk_db="5b1ff1fd0f58e8e4115fd5f72e0abaac"; 6333762c95037d16=EdzjRfZ3Zr5SC302n066a4ia%2FgCH1F6W%2FngRSjc5BaNdYg6vXdtdw6xQf89cKjdOWnwkP94E5p0y20crKMAJ8QRmHtI3jJN8MPz4sUZDIz9fNWZz896ux5HYPBdDCEqE6kPuKEHD9BY6BK%2FBe6o4jtJaXZDZtYFg0jakxJNb7WyHz%2F5AHmC6pciCGu79ofh0aYrbtrUI8cEXYGdGDiL%2F%2BN5wPcx%2BT%2BJ4GHmm40JJ4riqgIZhP3Y887ajnqsT97mU108xmZCc4Cdte2OBVqg3zAVxVbw8LO8X1yprI3RZ9cZYEzzjfkOqWw%3D%3D; __utmt=1; __utmb=30149280.24.9.1761229217624; _TDID_CK=1761229219533'

# --------------------------
# çˆ¬å–å‚æ•°
# --------------------------
TOP250_PAGES = 10  # 10é¡µ=250éƒ¨ç”µå½±
MAX_PAGES_PER_MOVIE = 10  # æ¯éƒ¨ç”µå½±çˆ¬10é¡µè¯„è®ºï¼ˆ200æ¡ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
RETRY_TIMES = 3  # å¤±è´¥é‡è¯•æ¬¡æ•°


# --------------------------
# å·¥å…·å‡½æ•°ï¼ˆåçˆ¬+é‡è¯•ï¼‰
# --------------------------
def retry_decorator(func):
    """å¤±è´¥é‡è¯•è£…é¥°å™¨ï¼ˆè‡ªåŠ¨é‡è¯•3æ¬¡ï¼‰"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        for i in range(RETRY_TIMES):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.warning(f"ç¬¬{i+1}æ¬¡å°è¯•å¤±è´¥ï¼š{str(e)}ï¼Œå°†é‡è¯•...")
                time.sleep(random.uniform(5, 10))  # é‡è¯•å‰å»¶è¿Ÿ
        logger.error(f"è¶…è¿‡{RETRY_TIMES}æ¬¡é‡è¯•ï¼Œæ”¾å¼ƒè¯¥ä»»åŠ¡")
        return None
    return wrapper


def get_random_headers():
    """éšæœºç”Ÿæˆè¯·æ±‚å¤´ï¼ˆå«User-Agentå’ŒCookieï¼‰"""
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/129.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) Safari/605.1.15",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) Safari/605.1.15",
        "Mozilla/5.0 (Linux; Android 14; Pixel 8) Chrome/128.0.0.0 Mobile Safari/537.36"
    ]
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Cookie": DOUBAN_COOKIE,
        "Referer": "https://movie.douban.com/top250",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9"
    }


# --------------------------
# æ•°æ®åº“æ“ä½œ
# --------------------------
def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼ˆåˆ›å»ºåº“å’Œè¡¨ï¼‰"""
    try:
        conn = pymysql.connect(
            host=DB_CONFIG["host"],
            user=DB_CONFIG["user"],
            password=DB_CONFIG["password"],
            charset=DB_CONFIG["charset"]
        )
        cursor = conn.cursor()
        # åˆ›å»ºæ•°æ®åº“
        cursor.execute(f"CREATE DATABASE IF NOT EXISTS {DB_CONFIG['database']}")
        cursor.execute(f"USE {DB_CONFIG['database']}")
        # åˆ›å»ºè¯„è®ºè¡¨
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS top250_comments (
            id INT AUTO_INCREMENT PRIMARY KEY,
            movie_id VARCHAR(20) NOT NULL,
            movie_rank INT DEFAULT 0,
            star INT NOT NULL,
            content TEXT NOT NULL,
            crawl_time DATETIME NOT NULL,
            INDEX idx_movie_id (movie_id)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """
        cursor.execute(create_table_sql)
        conn.commit()
        logger.success("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼ˆè¡¨ï¼štop250_commentsï¼‰")
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
        raise
    finally:
        if 'conn' in locals():
            conn.close()


def save_to_mysql(comments):
    """æ‰¹é‡ä¿å­˜è¯„è®ºåˆ°MySQL"""
    if not comments:
        return
    try:
        conn = pymysql.connect(**DB_CONFIG)
        cursor = conn.cursor()
        sql = """
        INSERT INTO top250_comments (movie_id, movie_rank, star, content, crawl_time)
        VALUES (%s, %s, %s, %s, %s)
        """
        data = [
            (c["movie_id"], c["movie_rank"], c["star"], c["content"], c["crawl_time"])
            for c in comments
        ]
        cursor.executemany(sql, data)
        conn.commit()
        logger.success(f"æˆåŠŸæ’å…¥{len(comments)}æ¡è¯„è®ºåˆ°æ•°æ®åº“")
    except Exception as e:
        conn.rollback()
        logger.error(f"æ•°æ®åº“æ’å…¥å¤±è´¥ï¼š{str(e)}")
    finally:
        if 'conn' in locals():
            conn.close()


# --------------------------
# æ ¸å¿ƒçˆ¬è™«é€»è¾‘ï¼ˆé€‚é…Top250 HTMLç»“æ„ï¼‰
# --------------------------
@retry_decorator
def get_top250_movie_ids(pages=10):
    """è·å–è±†ç“£Top250ç”µå½±IDå’Œæ’å"""
    top250_movies = []
    base_url = "https://movie.douban.com/top250?start={start}&filter="
    
    for page in range(pages):
        start = page * 25
        url = base_url.format(start=start)
        headers = get_random_headers()
        
        try:
            time.sleep(random.uniform(2, 4))
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ğŸ”´ å…³é”®ï¼šæ ¹æ®HTMLç»“æ„ï¼Œç”µå½±æ¡ç›®åœ¨class="grid_view"çš„olä¸‹çš„liä¸­
            movie_items = soup.select(".grid_view li .item")
            if not movie_items:
                logger.warning(f"ç¬¬{page+1}é¡µæœªæ‰¾åˆ°ç”µå½±æ¡ç›®ï¼Œå¯èƒ½é¡µé¢ç»“æ„æ›´æ–°")
                continue
            
            for idx, item in enumerate(movie_items):
                # æå–æ’åï¼ˆç¬¬1é¡µç¬¬1ä¸ªæ˜¯ç¬¬1åï¼Œç¬¬2ä¸ªæ˜¯ç¬¬2å...ï¼‰
                movie_rank = page * 25 + idx + 1
                # æå–ç”µå½±é“¾æ¥
                movie_link = item.select_one(".hd a")["href"]
                movie_id = re.search(r"/subject/(\d+)/", movie_link).group(1)
                top250_movies.append({"movie_id": movie_id, "movie_rank": movie_rank})
            
            logger.success(f"ç¬¬{page+1}é¡µçˆ¬å–å®Œæˆï¼Œå·²è·å–{len(movie_items)}éƒ¨ç”µå½±ID")
        
        except Exception as e:
            logger.error(f"ç¬¬{page+1}é¡µçˆ¬å–å¤±è´¥ï¼š{str(e)}")
            continue
    
    logger.success(f"å…±è·å–{len(top250_movies)}éƒ¨Top250ç”µå½±ID")
    return top250_movies


@retry_decorator
def crawl_movie_comments(movie_id, movie_rank, max_pages=10):
    """çˆ¬å–å•éƒ¨ç”µå½±çš„è¯„è®º"""
    comments = []
    url_template = f"https://movie.douban.com/subject/{movie_id}/comments?start={{start}}&limit=20&status=P"
    
    for page in range(max_pages):
        start = page * 20
        url = url_template.format(start=start)
        headers = get_random_headers()
        
        try:
            time.sleep(random.uniform(1, 3))
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # å¤„ç†éªŒè¯ç ï¼ˆæ‰‹åŠ¨å¹²é¢„ï¼‰
            if "éªŒè¯ç " in response.text:
                logger.warning(f"å‡ºç°éªŒè¯ç ï¼è¯·æ‰“å¼€ä»¥ä¸‹é“¾æ¥éªŒè¯ï¼š\n{url}")
                input("éªŒè¯å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
                response = requests.get(url, headers=headers)
            
            soup = BeautifulSoup(response.text, "html.parser")
            comment_items = soup.find_all("div", class_="comment-item")
            
            for item in comment_items:
                # æå–æ˜Ÿçº§
                rating_tag = item.find("span", class_="rating")
                star = int(rating_tag["class"][0].replace("allstar", "")) // 10 if rating_tag else 0
                # æå–è¯„è®ºå†…å®¹
                content_tag = item.find("span", class_="short")
                content = content_tag.text.strip() if content_tag else ""
                
                if content:
                    comments.append({
                        "movie_id": movie_id,
                        "movie_rank": movie_rank,
                        "star": star,
                        "content": content,
                        "crawl_time": time.strftime("%Y-%m-%d %H:%M:%S")
                    })
            
            print(f"ç”µå½±{movie_id}ï¼šç¬¬{page+1}é¡µçˆ¬å–å®Œæˆï¼Œç´¯è®¡{len(comments)}æ¡è¯„è®º")
        
        except Exception as e:
            print(f"çˆ¬å–ç¬¬{page+1}é¡µå¤±è´¥ï¼š{str(e)}")
            continue
    
    return comments


# --------------------------
# ä¸»æµç¨‹
# --------------------------
def main():
    # 1. åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    
    # 2. è·å–Top250ç”µå½±ID
    top250_movies = get_top250_movie_ids(pages=TOP250_PAGES)
    if not top250_movies:
        logger.error("æœªè·å–åˆ°ç”µå½±IDï¼Œç¨‹åºç»ˆæ­¢")
        return
    
    # 3. çˆ¬å–æ¯éƒ¨ç”µå½±çš„è¯„è®ºå¹¶ä¿å­˜åˆ°æ•°æ®åº“
    for movie in top250_movies:
        movie_id = movie["movie_id"]
        movie_rank = movie["movie_rank"]
        
        logger.info(f"\nå¼€å§‹çˆ¬å–Top250ç¬¬{movie_rank}åï¼ˆID:{movie_id}ï¼‰")
        comments = crawl_movie_comments(
            movie_id=movie_id,
            movie_rank=movie_rank,
            max_pages=MAX_PAGES_PER_MOVIE
        )
        
        if comments:
            save_to_mysql(comments)
        
        # ç”µå½±é—´å»¶è¿Ÿï¼ˆåçˆ¬ï¼‰
        time.sleep(random.uniform(5, 10))
    
    logger.success("âœ… æ‰€æœ‰Top250ç”µå½±å½±è¯„çˆ¬å–å®Œæˆï¼")


if __name__ == "__main__":
    main()